{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theodore's paths (do not run this cell)\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1,org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 --jars spark-sql-kafka-0-10_2.11-2.4.1.jar pyspark-shell'\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDlHETL1X7p_"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"/home/user1/trainig_material/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "#os.environ[\"HIVE_HOME\"] =\"/usr/lib/hive\"\n",
    "#os.environ[\"PATH\"] =os.environ[\"HIVE_HOME\"]+\"/bin\"\n",
    "#os.environ[\"JAVA_HOME\"]=\"/usr/java/jdk1.7.0_67-cloudera\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/home/cloudera/anaconda3/bin/python3.7\" \n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/cloudera/anaconda3/bin/python3.7\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFQofpu7X7qB"
   },
   "outputs": [],
   "source": [
    "# Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "# Kafka\n",
    "#from pyspark.streaming.kafka import KafkaUtils\n",
    "# json parsing\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext,HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiYoHyp8X7qC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "houVHeHYX7qC"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"twitter\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0wK5WUgX7qC"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B87w91i2X7qD"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KKNcpYpX7qD"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjlHcIfuX7qF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from textblob import TextBlob\n",
    "\n",
    "from kafka import KafkaClient\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import TopicPartition\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not utilizing a structured streaming environment, this will allow us to save tweets in consumer \n",
    "#nd not neet to run producer and streaming simultaneosly avoiding error:420\n",
    "consumer = KafkaConsumer('final_twitter', \n",
    "                         group_id='1', # Using group so that it doesn't re-read messages\n",
    "                         auto_offset_reset='earliest', \n",
    "                         enable_auto_commit=True, \n",
    "                         bootstrap_servers=['localhost:9092'], \n",
    "                         value_deserializer=lambda m: json.loads(m))\n",
    "# Create TopicPartition object so we can keep track of offset and know when to exit consumer loop later\n",
    "tp = TopicPartition('final_twitter',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [] # Put in separate cell so we don't clear rows every time we run next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UESdeAulX7qF",
    "outputId": "a3dad678-dad9-4baa-a688-366f72dc023b"
   },
   "outputs": [],
   "source": [
    "date_format = '%a %b %d %H:%M:%S %Y' # To convert created_at to Datetime object\n",
    "\n",
    "#list of dicts, this will save tweets from consumer\n",
    "for msg in consumer:\n",
    "    # Obtain the last offset value, so that we know when to exit loop\n",
    "    current_position = consumer.position(tp) # Store current offset pos so we can seek to it after\n",
    "    consumer.seek_to_end(tp)\n",
    "    last_offset = consumer.position(tp)\n",
    "    \n",
    "    consumer.seek(tp, current_position) # Seek back to where we were\n",
    "    print(\"\\nCurrent offset:\", msg.offset)\n",
    "    print(\"Last offset:\", last_offset)\n",
    "    \n",
    "    # Comment this out to continue even if consumer catches up to producer\n",
    "    if msg.offset >= last_offset - 1: \n",
    "        break\n",
    "    \n",
    "    # this converts consumer to type dict\n",
    "    row = msg.value\n",
    "    # pull the hashtags\n",
    "    if len(row['entities']['hashtags']) > 0:\n",
    "        #hashtag_text = [row['entities']['hashtags'][0]['text'] # Get text string of first hashtag\n",
    "        hashtag_text = [row['entities']['hashtags'][i]['text'].lower() for i, tag in enumerate(row['entities']['hashtags'])]\n",
    "    else:\n",
    "        #continue # Commented this out since if we continue, the next line wouldn't execute and we wouldn't store the tweet\n",
    "        hashtag_text = []\n",
    "    # create a row and add it to the list, we only need Tweet Id, User Id, Text, hashtags, and event time\n",
    "    row = Row(TweetID=row['id'], \n",
    "              Text=row['text'], \n",
    "              UserID=row['user']['id'], \n",
    "              Hashtags=hashtag_text,\n",
    "              #Event_time=row['created_at']\n",
    "              Event_time=datetime.strptime(row['created_at'].replace('+0000 ', ''), date_format)\n",
    "    )\n",
    "    rows.append(row)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74QEdb5nX7qG"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(rows[:10]) # First 10 rows\n",
    "print('\\nNumber of tweets collected: {}'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of dicts to a spark dataframe as per the requirements\n",
    "df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of tweets in last 30 second window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweet_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably won't capture anything unless run right away, since it uses current time instead of most recent tweet\n",
    "# Maybe better to use below method\n",
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) as num_tweets_last_30\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between (current_timestamp() - INTERVAL 30 SECOND ) \n",
    "                                AND current_timestamp()\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent tweet event_time captured, store as datetime\n",
    "# We can use this for all other queries to compare against most recent tweet (instead of current time, since it might be delayed)\n",
    "most_recent_tweet = spark.sql(\"\"\"\n",
    "    SELECT Event_time\n",
    "    FROM tweet_df\n",
    "    ORDER BY Event_time \n",
    "    DESC LIMIT 1\"\"\").collect()[0].Event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datetime of most recently captured tweet is: {}'.format(most_recent_tweet)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "    SELECT COUNT(*) as Num_Tweets_Last_30_From_Newest_Tweet\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "print(query)\n",
    "num_tweets_last_30 = spark.sql(query)\n",
    "num_tweets_last_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num active users last 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual last 30 seconds from current time\n",
    "query = \"\"\"\n",
    "    SELECT count(distinct(UserID)) as Num_Active_Users_Last_30_From_Current\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between (current_timestamp() - INTERVAL 30 SECOND ) \n",
    "                                AND current_timestamp()\"\"\".format(most_recent_tweet)\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 seconds prior to most recently received tweet\n",
    "query = \"\"\"\n",
    "    SELECT count(distinct(UserID)) as Num_Active_Users_Last_30_From_Newest_Tweet\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "print(query)\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most used hashtag in last 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key=lambda a: a[1] Means use the second value in the (word, num_occurrences) tuple as comparison key for max\n",
    "query = \"\"\"\n",
    "    SELECT Hashtags\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "df_last_30 = spark.sql(query)\n",
    "frequent_hashtag_tup_last_30 = max(df_last_30.select(['Hashtags']).rdd.flatMap(lambda a: a.Hashtags).countByValue().items(), key=lambda a: a[1])\n",
    "print('Most frequently occuring hashtag in last 30 seconds: {}'.format(frequent_hashtag_tup_last_30[0]))\n",
    "print('Num occurrences: {}'.format(frequent_hashtag_tup_last_30[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most used hashtag in complete time of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key=lambda a: a[1] Means use the second value in the (word, num_occurrences) tuple as comparison key for max\n",
    "frequent_hashtag_tup_complete = max(df.select(['Hashtags']).rdd.flatMap(lambda a: a.Hashtags).countByValue().items(), key=lambda a: a[1])\n",
    "print('Most frequently occuring hashtag in complete time of analysis: {}'.format(frequent_hashtag_tup_complete[0]))\n",
    "print('Num occurrences: {}'.format(frequent_hashtag_tup_complete[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average of a given Hashtag every 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import explode\n",
    "# I basically followed this tutorial: \n",
    "# https://www.linkedin.com/pulse/time-series-moving-average-apache-pyspark-laurent-weichberger/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"seven day\" Window from seven days previous to the current day (row), using previous casting of timestamp to long (number of seconds).\n",
    "\n",
    "# remember a start value of \"-1\" means one off before the current row, and we are taking the timestamp as a long and comparing it to the rangeBetween amount of time.\n",
    "\n",
    "# Remember an end value of 0 is the current row.\n",
    "# We use 6 days because rangeBetween is inclusive of start & end values.\n",
    "# Thank you Dr. Kevin Maguire for the bug fix!\n",
    "#windowSpec = Window.orderBy(func.col(\"Event_time\").cast('long')).rangeBetween(2, 0) # Using 3 seconds for now. Set to 59 for 1 minute\n",
    "windowSpec = Window.orderBy(func.col(\"Event_time\").cast('long')).rangeBetween(2, 0) # Using 3 seconds for now. Set to 59 for 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the dataframe so that each element in each Hashtag array has its own row, \n",
    "# and so that the Hashtags column now only contains one hashtag string\n",
    "df_exploded = df.withColumn('Hashtags', explode('Hashtags'))\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a new column which has the count of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DF such that it only contains rows with our hashtag of interest\n",
    "df2 = df_exploded.filter(df_exploded.Hashtags == frequent_hashtag_tup_complete[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df2 = df2.withColumn('count', lit(1.0)) # Simply add a new column and set each val to 1 (used for moving avg later)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit: Not sure if we need this\n",
    "# We do a wordcount on Event_time, because at this point each Event_time row is one occurance for the most frequent hashtag\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as f\n",
    "df3 = df2 \\\n",
    "        .groupBy('Event_time') \\\n",
    "        .count() \\\n",
    "        .sort('Event_time', ascending=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSQL method - not sure if this is correct\n",
    "df3.createOrReplaceTempView(\"df3\")\n",
    "spark.sql(\n",
    "    \"\"\"SELECT *, mean(count) OVER (\n",
    "        ORDER BY CAST(Event_time AS timestamp) \n",
    "        RANGE BETWEEN INTERVAL 60 SECONDS PRECEDING AND CURRENT ROW\n",
    "     ) AS Moving_Average FROM df3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried other method but not sure why this returns null. Not needed since above works\n",
    "\n",
    "df4 = df3.withColumn('Moving Average', func.avg(\"count\").over(windowSpec)) \n",
    "print('Showing moving average for hashtag: {}'.format(frequent_hashtag_tup_complete[0]))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write df to hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add timestamp to this record in case we want to make sense of the data in the hive table later\n",
    "# Also, reorder columns so timestamp is on left\n",
    "table_to_write = num_tweets_last_30.withColumn('End_of_Capturing_Window', lit(most_recent_tweet))\n",
    "table_to_write = table_to_write.select('End_of_Capturing_Window', 'Num_Tweets_Last_30_From_Newest_Tweet')\n",
    "table_to_write.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create db and table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE twitter_analysis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    USE twitter_analysis\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets_last_30(\n",
    "        End_of_Capturing_Window String, \n",
    "        Num_Tweets_Last_30_From_Newest_Tweet String\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_to_write.write.mode(\"append\").format('hive').saveAsTable(\"twitter_analysis.tweets_last_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM tweets_last_30\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.7.1",
   "language": "python",
   "name": "python3.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
