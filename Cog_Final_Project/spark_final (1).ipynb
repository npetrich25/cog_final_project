{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theodore's paths (do not run this cell)\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1,org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 --jars spark-sql-kafka-0-10_2.11-2.4.1.jar pyspark-shell'\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CDlHETL1X7p_"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"/home/user1/trainig_material/spark-2.4.4-bin-hadoop2.7\"\n",
    "#os.environ[\"SPARK_HOME\"]='/usr/local/bin/spark-2.4.4-bin-hadoop2.6'\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "#os.environ[\"HIVE_HOME\"] =\"/usr/lib/hive\"\n",
    "#os.environ[\"PATH\"] =os.environ[\"HIVE_HOME\"]+\"/bin\"\n",
    "#os.environ[\"JAVA_HOME\"]=\"/usr/java/jdk1.7.0_67-cloudera\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/home/cloudera/anaconda3/bin/python3.7\" \n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/cloudera/anaconda3/bin/python3.7\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_292\n",
      "Branch \n",
      "Compiled by user  on 2019-08-27T21:21:38Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aFQofpu7X7qB"
   },
   "outputs": [],
   "source": [
    "# Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "# Kafka\n",
    "#from pyspark.streaming.kafka import KafkaUtils\n",
    "# json parsing\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext,HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "houVHeHYX7qC"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"twitter\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g0wK5WUgX7qC"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B87w91i2X7qD"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4KKNcpYpX7qD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|    databaseName|\n",
      "+----------------+\n",
      "|         default|\n",
      "|            test|\n",
      "|twitter_analysis|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AjlHcIfuX7qF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from textblob import TextBlob\n",
    "\n",
    "from kafka import KafkaClient\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import TopicPartition\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not utilizing a structured streaming environment, this will allow us to save tweets in consumer \n",
    "#nd not neet to run producer and streaming simultaneosly avoiding error:420\n",
    "consumer = KafkaConsumer('final_twitter', \n",
    "                         group_id='1', # Using group so that it doesn't re-read messages\n",
    "                         auto_offset_reset='earliest', \n",
    "                         enable_auto_commit=True, \n",
    "                         bootstrap_servers=['localhost:9092'], \n",
    "                         value_deserializer=lambda m: json.loads(m))\n",
    "# Create TopicPartition object so we can keep track of offset and know when to exit consumer loop later\n",
    "tp = TopicPartition('final_twitter',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [] # Put in separate cell so we don't clear rows every time we run next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "UESdeAulX7qF",
    "outputId": "a3dad678-dad9-4baa-a688-366f72dc023b"
   },
   "outputs": [],
   "source": [
    "date_format = '%a %b %d %H:%M:%S %Y' # To convert created_at to Datetime object\n",
    "\n",
    "#list of dicts, this will save tweets from consumer\n",
    "for msg in consumer:\n",
    "    # Obtain the last offset value, so that we know when to exit loop\n",
    "    current_position = consumer.position(tp) # Store current offset pos so we can seek to it after\n",
    "    consumer.seek_to_end(tp)\n",
    "    last_offset = consumer.position(tp)\n",
    "    \n",
    "    consumer.seek(tp, current_position) # Seek back to where we were\n",
    "    #print(\"\\nCurrent offset:\", msg.offset)\n",
    "    #print(\"Last offset:\", last_offset)\n",
    "    \n",
    "    # Comment this out to continue even if consumer catches up to producer\n",
    "    if msg.offset >= last_offset - 1: \n",
    "        break\n",
    "    \n",
    "    # this converts consumer to type dict\n",
    "    row = msg.value\n",
    "    \n",
    "    # pull the hashtags\n",
    "    if len(row['entities']['hashtags']) > 0:\n",
    "        #hashtag_text = [row['entities']['hashtags'][0]['text'] # Get text string of first hashtag\n",
    "        hashtag_text = [row['entities']['hashtags'][i]['text'].lower() for i, tag in enumerate(row['entities']['hashtags'])]\n",
    "    else:\n",
    "        #continue # Commented this out since if we continue, the next line wouldn't execute and we wouldn't store the tweet\n",
    "        hashtag_text = []\n",
    "    # create a row and add it to the list, we only need Tweet Id, User Id, Text, hashtags, and event time\n",
    "    row = Row(TweetID=row['id'], \n",
    "              Text=row['text'], \n",
    "              UserID=row['user']['id'], \n",
    "              Hashtags=hashtag_text,\n",
    "              #Event_time=row['created_at']\n",
    "              Event_time=datetime.strptime(row['created_at'].replace('+0000 ', ''), date_format)\n",
    "    )\n",
    "    \n",
    "    rows.append(row)\n",
    "    #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "74QEdb5nX7qG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 49), Hashtags=[], Text='Speechless.', TweetID=1420412688723693571, UserID=906472343857328128), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 49), Hashtags=[], Text='RT @ijaicool: Lee Zii Jia at the Olympics: https://t.co/zFL96N028m', TweetID=1420412689319223297, UserID=868734692232581120), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 49), Hashtags=[], Text='I wish I was this wise and strong when I was her age', TweetID=1420412689747226631, UserID=378120117), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 49), Hashtags=[], Text='KE olympics au fic when?', TweetID=1420412689818562561, UserID=1229910244203139072), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=[], Text='Olympics?\\nMore like Olamepics. Amirite?', TweetID=1420412690380382210, UserID=64345412), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=[], Text=\"RT @BrianBertie1998: It's over! Kento Momota, the world #1 and undisputed favourite to win the gold in his own home, is defeated and knocke…\", TweetID=1420412691118624768, UserID=1311798800231489538), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=['olympics'], Text='RT @binance: Would you long or short this #Olympics mountain bike course chart? https://t.co/itZImuML8y', TweetID=1420412691890507777, UserID=1346472473240354817), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=[], Text='@JeremyDBoreing  a comedian breaks down the Olympics like no one else can. Enjoy and ReTweet? 🇺🇸 \\n\\nthis one is a mu… https://t.co/kTzCdYr0gt', TweetID=1420412692129484801, UserID=66593978), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=['桥本大辉'], Text='#桥本大辉 Tokyo Olympics is not https://t.co/0WARApqYZz Olympics is not https://t.co/0WARApqYZz Olympics is not… https://t.co/oYesDM6sUd', TweetID=1420412691911299074, UserID=1239370228192153600), Row(Event_time=datetime.datetime(2021, 7, 28, 15, 55, 50), Hashtags=[], Text=\"RT @CBCOlympics: Without saying a word, Luciana Alvarado's message at Tokyo 2020 was loud and clear\\nhttps://t.co/75SwXjTGas\", TweetID=1420412692846714889, UserID=1030096674046103552)]\n",
      "\n",
      "Number of tweets collected: 400\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(rows[:10]) # First 10 rows\n",
    "print('\\nNumber of tweets collected: {}'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of dicts to a spark dataframe as per the requirements\n",
    "df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------------+-------------------+-------------------+\n",
      "|         Event_time|  Hashtags|                     Text|            TweetID|             UserID|\n",
      "+-------------------+----------+-------------------------+-------------------+-------------------+\n",
      "|2021-07-28 15:55:49|        []|              Speechless.|1420412688723693571| 906472343857328128|\n",
      "|2021-07-28 15:55:49|        []|     RT @ijaicool: Lee...|1420412689319223297| 868734692232581120|\n",
      "|2021-07-28 15:55:49|        []|     I wish I was this...|1420412689747226631|          378120117|\n",
      "|2021-07-28 15:55:49|        []|     KE olympics au fi...|1420412689818562561|1229910244203139072|\n",
      "|2021-07-28 15:55:50|        []|     Olympics?\n",
      "More li...|1420412690380382210|           64345412|\n",
      "|2021-07-28 15:55:50|        []|     RT @BrianBertie19...|1420412691118624768|1311798800231489538|\n",
      "|2021-07-28 15:55:50|[olympics]|     RT @binance: Woul...|1420412691890507777|1346472473240354817|\n",
      "|2021-07-28 15:55:50|        []|     @JeremyDBoreing  ...|1420412692129484801|           66593978|\n",
      "|2021-07-28 15:55:50|[桥本大辉]| #桥本大辉 Tokyo Olymp...|1420412691911299074|1239370228192153600|\n",
      "|2021-07-28 15:55:50|        []|     RT @CBCOlympics: ...|1420412692846714889|1030096674046103552|\n",
      "|2021-07-28 15:55:50|[yiayflag]|     RT @jacksfilms: N...|1420412692960055296|1316384831371120643|\n",
      "|2021-07-28 15:55:50|        []|     RT @RajatSethi86:...|1420412693001871371|         2371204105|\n",
      "|2021-07-28 15:55:50|        []|     I dislike Shapiro...|1420412692922355712|1400608723488362501|\n",
      "|2021-07-28 15:55:50|        []|     I have a question...|1420412694142857220|1420411388715933699|\n",
      "|2021-07-28 15:55:51|[桥本大辉]|#桥本大辉\n",
      "olympics？ja...|1420412694801174532|1336340752280870915|\n",
      "|2021-07-28 15:55:51|        []|     RT @seulcrescent:...|1420412695199686656|         3242655404|\n",
      "|2021-07-28 15:55:51|        []|     @CNN @ChrisCilliz...|1420412695166205963|1347258174475280386|\n",
      "|2021-07-28 15:55:51|    [yuta]|     RT @0SAKl: Soccer...|1420412696101490692|          538888700|\n",
      "|2021-07-28 15:55:51|        []|     RT @lindseyadler:...|1420412696361639936| 751268842911916037|\n",
      "|2021-07-28 15:55:51|        []|     Respect to Simone...|1420412696395190279|          950965892|\n",
      "+-------------------+----------+-------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of tweets in last 30 second window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweet_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|num_tweets_last_30|\n",
      "+------------------+\n",
      "|                 0|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probably won't capture anything unless run right away, since it uses current time instead of most recent tweet\n",
    "# Maybe better to use below method\n",
    "spark.sql(\"\"\"\n",
    "    SELECT count(*) as num_tweets_last_30\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between (current_timestamp() - INTERVAL 30 SECOND ) \n",
    "                                AND current_timestamp()\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent tweet event_time captured, store as datetime\n",
    "# We can use this for all other queries to compare against most recent tweet (instead of current time, since it might be delayed)\n",
    "most_recent_tweet = spark.sql(\"\"\"\n",
    "    SELECT Event_time\n",
    "    FROM tweet_df\n",
    "    ORDER BY Event_time \n",
    "    DESC LIMIT 1\"\"\").collect()[0].Event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime of most recently captured tweet is: 2021-07-28 17:54:56\n"
     ]
    }
   ],
   "source": [
    "print('Datetime of most recently captured tweet is: {}'.format(most_recent_tweet)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT COUNT(*) as Num_Tweets_Last_30_From_Newest_Tweet\n",
      "    FROM tweet_df\n",
      "    WHERE Event_time between ('2021-07-28 17:54:56' - INTERVAL 30 SECOND ) \n",
      "                                AND '2021-07-28 17:54:56'\n",
      "+------------------------------------+\n",
      "|Num_Tweets_Last_30_From_Newest_Tweet|\n",
      "+------------------------------------+\n",
      "|                                 231|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "    SELECT COUNT(*) as Num_Tweets_Last_30_From_Newest_Tweet\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "print(query)\n",
    "num_tweets_last_30 = spark.sql(query)\n",
    "num_tweets_last_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num active users last 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|Num_Active_Users_Last_30_From_Current|\n",
      "+-------------------------------------+\n",
      "|                                    0|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual last 30 seconds from current time\n",
    "query = \"\"\"\n",
    "    SELECT count(distinct(UserID)) as Num_Active_Users_Last_30_From_Current\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between (current_timestamp() - INTERVAL 30 SECOND ) \n",
    "                                AND current_timestamp()\"\"\".format(most_recent_tweet)\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT count(distinct(UserID)) as Num_Active_Users_Last_30_From_Newest_Tweet\n",
      "    FROM tweet_df\n",
      "    WHERE Event_time between ('2021-07-28 15:16:35' - INTERVAL 30 SECOND ) \n",
      "                                AND '2021-07-28 15:16:35'\n",
      "+------------------------------------------+\n",
      "|Num_Active_Users_Last_30_From_Newest_Tweet|\n",
      "+------------------------------------------+\n",
      "|                                       387|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 30 seconds prior to most recently received tweet\n",
    "query = \"\"\"\n",
    "    SELECT count(distinct(UserID)) as Num_Active_Users_Last_30_From_Newest_Tweet\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "print(query)\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most used hashtag in last 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently occuring hashtag in last 30 seconds: olympics\n",
      "Num occurrences: 58\n"
     ]
    }
   ],
   "source": [
    "# key=lambda a: a[1] Means use the second value in the (word, num_occurrences) tuple as comparison key for max\n",
    "query = \"\"\"\n",
    "    SELECT Hashtags\n",
    "    FROM tweet_df\n",
    "    WHERE Event_time between ('{}' - INTERVAL 30 SECOND ) \n",
    "                                AND '{}'\"\"\".format(most_recent_tweet, most_recent_tweet)\n",
    "df_last_30 = spark.sql(query)\n",
    "frequent_hashtag_tup_last_30 = max(df_last_30.select(['Hashtags']).rdd.flatMap(lambda a: a.Hashtags).countByValue().items(), key=lambda a: a[1])\n",
    "print('Most frequently occuring hashtag in last 30 seconds: {}'.format(frequent_hashtag_tup_last_30[0]))\n",
    "print('Num occurrences: {}'.format(frequent_hashtag_tup_last_30[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most used hashtag in complete time of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently occuring hashtag in complete time of analysis: olympics\n",
      "Num occurrences: 94\n"
     ]
    }
   ],
   "source": [
    "# key=lambda a: a[1] Means use the second value in the (word, num_occurrences) tuple as comparison key for max\n",
    "frequent_hashtag_tup_complete = max(df.select(['Hashtags']).rdd.flatMap(lambda a: a.Hashtags).countByValue().items(), key=lambda a: a[1])\n",
    "print('Most frequently occuring hashtag in complete time of analysis: {}'.format(frequent_hashtag_tup_complete[0]))\n",
    "print('Num occurrences: {}'.format(frequent_hashtag_tup_complete[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average of a given Hashtag every 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import explode\n",
    "# I basically followed this tutorial: \n",
    "# https://www.linkedin.com/pulse/time-series-moving-average-apache-pyspark-laurent-weichberger/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"seven day\" Window from seven days previous to the current day (row), using previous casting of timestamp to long (number of seconds).\n",
    "\n",
    "# remember a start value of \"-1\" means one off before the current row, and we are taking the timestamp as a long and comparing it to the rangeBetween amount of time.\n",
    "\n",
    "# Remember an end value of 0 is the current row.\n",
    "# We use 6 days because rangeBetween is inclusive of start & end values.\n",
    "# Thank you Dr. Kevin Maguire for the bug fix!\n",
    "#windowSpec = Window.orderBy(func.col(\"Event_time\").cast('long')).rangeBetween(2, 0) # Using 3 seconds for now. Set to 59 for 1 minute\n",
    "windowSpec = Window.orderBy(func.col(\"Event_time\").cast('long')).rangeBetween(2, 0) # Using 3 seconds for now. Set to 59 for 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+--------------------+-------------------+-------------------+\n",
      "|         Event_time|              Hashtags|                Text|            TweetID|             UserID|\n",
      "+-------------------+----------------------+--------------------+-------------------+-------------------+\n",
      "|2021-07-28 14:52:41|        winitwednesday|RT @HelleniaUK: #...|1420396798322040832|          348526826|\n",
      "|2021-07-28 14:52:41|           competition|RT @HelleniaUK: #...|1420396798322040832|          348526826|\n",
      "|2021-07-28 14:52:41|      blacklivesmatter|RT @StanisElsborg...|1420396799014150147|         2438109488|\n",
      "|2021-07-28 14:52:41|          raisearacket|RT @bwfmedia: The...|1420396799454588930|1390696881202688000|\n",
      "|2021-07-28 14:52:41|             tokyo2020|RT @bwfmedia: The...|1420396799454588930|1390696881202688000|\n",
      "|2021-07-28 14:52:41|              olympics|RT @bwfmedia: The...|1420396799454588930|1390696881202688000|\n",
      "|2021-07-28 14:52:41|          shameonjapan|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|\n",
      "|2021-07-28 14:52:41|              olympics|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|\n",
      "|2021-07-28 14:52:41|              体操競技|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|\n",
      "|2021-07-28 14:52:41|フェアプレー精神がない|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|\n",
      "|2021-07-28 14:52:41|                bronze|RT @Olympics: #Br...|1420396800146481158|         1409659945|\n",
      "|2021-07-28 14:52:41|                   ina|RT @Olympics: #Br...|1420396800146481158|         1409659945|\n",
      "|2021-07-28 14:52:41|                bronze|RT @Olympics: #Br...|1420396800146481158|         1409659945|\n",
      "|2021-07-28 14:52:41|         weightlifting|RT @Olympics: #Br...|1420396800146481158|         1409659945|\n",
      "|2021-07-28 14:52:41|      strongertogether|RT @Olympics: #Br...|1420396800146481158|         1409659945|\n",
      "|2021-07-28 14:52:41|                wonwoo|RT @astrowons: #W...|1420396802033913861| 942024346750500864|\n",
      "|2021-07-28 14:52:42|             tokyo2020|RT @ITFTennis: On...|1420396802512089088|          562036192|\n",
      "|2021-07-28 14:52:42|                tennis|RT @ITFTennis: On...|1420396802512089088|          562036192|\n",
      "|2021-07-28 14:52:42|              olympics|RT @ITFTennis: On...|1420396802512089088|          562036192|\n",
      "|2021-07-28 14:52:42|                wonwoo|RT @astrowons: #W...|1420396802818199553|1342741108288188417|\n",
      "+-------------------+----------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode the dataframe so that each element in each Hashtag array has its own row, \n",
    "# and so that the Hashtags column now only contains one hashtag string\n",
    "df_exploded = df.withColumn('Hashtags', explode('Hashtags'))\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a new column which has the count of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DF such that it only contains rows with our hashtag of interest\n",
    "df2 = df_exploded.filter(df_exploded.Hashtags == frequent_hashtag_tup_complete[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+-------------------+-------------------+\n",
      "|         Event_time|Hashtags|                Text|            TweetID|             UserID|\n",
      "+-------------------+--------+--------------------+-------------------+-------------------+\n",
      "|2021-07-28 14:52:41|olympics|RT @bwfmedia: The...|1420396799454588930|1390696881202688000|\n",
      "|2021-07-28 14:52:41|olympics|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|\n",
      "|2021-07-28 14:52:42|olympics|RT @ITFTennis: On...|1420396802512089088|          562036192|\n",
      "|2021-07-28 14:52:42|olympics|Arabian iptv list...|1420396803841830915| 838513081529565184|\n",
      "|2021-07-28 14:52:42|olympics|RT @FeminaIndia: ...|1420396805402071040| 962504002641555457|\n",
      "|2021-07-28 14:52:42|olympics|RT @JioJio7907972...|1420396805976690694|1147786434901712896|\n",
      "|2021-07-28 14:52:42|olympics|RT @UN_Women: We ...|1420396806014390276|1055987824749625344|\n",
      "|2021-07-28 14:52:43|olympics|RT @2652450828Hi:...|1420396808312803331|1281456596732661761|\n",
      "|2021-07-28 15:15:53|olympics|#Olympics  #Tokyo...|1420402637183143938| 865245825735172096|\n",
      "|2021-07-28 15:15:53|olympics|RT @FIFAcom: 🔢 L...|1420402638319865857|           93503410|\n",
      "|2021-07-28 15:15:53|olympics|RT @sharathkamal1...|1420402638743433217|1412999294202761216|\n",
      "|2021-07-28 15:15:53|olympics|We  need  to  be ...|1420402640303759360|1150347101483745281|\n",
      "|2021-07-28 15:15:54|olympics|There's a lot to ...|1420402642526822413|         1694579766|\n",
      "|2021-07-28 15:15:54|olympics|RT @BadmintonTalk...|1420402643894108163| 893963736754429952|\n",
      "|2021-07-28 15:15:55|olympics|RT @SkySportsPL: ...|1420402644967989253|1406711531379867648|\n",
      "|2021-07-28 15:15:55|olympics|RT @Evelyn2time: ...|1420402647538966530|1198312292972027904|\n",
      "|2021-07-28 15:15:56|olympics|#Olympics Sports ...|1420402649791307787|1420393837579567111|\n",
      "|2021-07-28 15:15:56|olympics|RT @JulesBoykoff:...|1420402650554593280|         1367273863|\n",
      "|2021-07-28 15:15:56|olympics|RT @fakhrifadzli:...|1420402653012500482|1386659738528600068|\n",
      "|2021-07-28 15:15:57|olympics|RT @Cocora55: how...|1420402654925135881|1209417101368119296|\n",
      "+-------------------+--------+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+-------------------+-------------------+-----+\n",
      "|         Event_time|Hashtags|                Text|            TweetID|             UserID|count|\n",
      "+-------------------+--------+--------------------+-------------------+-------------------+-----+\n",
      "|2021-07-28 14:52:41|olympics|RT @bwfmedia: The...|1420396799454588930|1390696881202688000|  1.0|\n",
      "|2021-07-28 14:52:41|olympics|RT @DIN_NAYU: #sh...|1420396799655747591|1245984503098564608|  1.0|\n",
      "|2021-07-28 14:52:42|olympics|RT @ITFTennis: On...|1420396802512089088|          562036192|  1.0|\n",
      "|2021-07-28 14:52:42|olympics|Arabian iptv list...|1420396803841830915| 838513081529565184|  1.0|\n",
      "|2021-07-28 14:52:42|olympics|RT @FeminaIndia: ...|1420396805402071040| 962504002641555457|  1.0|\n",
      "|2021-07-28 14:52:42|olympics|RT @JioJio7907972...|1420396805976690694|1147786434901712896|  1.0|\n",
      "|2021-07-28 14:52:42|olympics|RT @UN_Women: We ...|1420396806014390276|1055987824749625344|  1.0|\n",
      "|2021-07-28 14:52:43|olympics|RT @2652450828Hi:...|1420396808312803331|1281456596732661761|  1.0|\n",
      "|2021-07-28 15:15:53|olympics|#Olympics  #Tokyo...|1420402637183143938| 865245825735172096|  1.0|\n",
      "|2021-07-28 15:15:53|olympics|RT @FIFAcom: 🔢 L...|1420402638319865857|           93503410|  1.0|\n",
      "|2021-07-28 15:15:53|olympics|RT @sharathkamal1...|1420402638743433217|1412999294202761216|  1.0|\n",
      "|2021-07-28 15:15:53|olympics|We  need  to  be ...|1420402640303759360|1150347101483745281|  1.0|\n",
      "|2021-07-28 15:15:54|olympics|There's a lot to ...|1420402642526822413|         1694579766|  1.0|\n",
      "|2021-07-28 15:15:54|olympics|RT @BadmintonTalk...|1420402643894108163| 893963736754429952|  1.0|\n",
      "|2021-07-28 15:15:55|olympics|RT @SkySportsPL: ...|1420402644967989253|1406711531379867648|  1.0|\n",
      "|2021-07-28 15:15:55|olympics|RT @Evelyn2time: ...|1420402647538966530|1198312292972027904|  1.0|\n",
      "|2021-07-28 15:15:56|olympics|#Olympics Sports ...|1420402649791307787|1420393837579567111|  1.0|\n",
      "|2021-07-28 15:15:56|olympics|RT @JulesBoykoff:...|1420402650554593280|         1367273863|  1.0|\n",
      "|2021-07-28 15:15:56|olympics|RT @fakhrifadzli:...|1420402653012500482|1386659738528600068|  1.0|\n",
      "|2021-07-28 15:15:57|olympics|RT @Cocora55: how...|1420402654925135881|1209417101368119296|  1.0|\n",
      "+-------------------+--------+--------------------+-------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df2 = df2.withColumn('count', lit(1.0)) # Simply add a new column and set each val to 1 (used for moving avg later)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|         Event_time|count|\n",
      "+-------------------+-----+\n",
      "|2021-07-28 14:52:41|    2|\n",
      "|2021-07-28 14:52:42|    5|\n",
      "|2021-07-28 14:52:43|    1|\n",
      "|2021-07-28 15:15:53|    4|\n",
      "|2021-07-28 15:15:54|    2|\n",
      "|2021-07-28 15:15:55|    2|\n",
      "|2021-07-28 15:15:56|    3|\n",
      "|2021-07-28 15:15:57|    1|\n",
      "|2021-07-28 15:15:58|    3|\n",
      "|2021-07-28 15:15:59|    1|\n",
      "|2021-07-28 15:16:00|    1|\n",
      "|2021-07-28 15:16:01|    1|\n",
      "|2021-07-28 15:16:02|    3|\n",
      "|2021-07-28 15:16:03|    1|\n",
      "|2021-07-28 15:16:04|    6|\n",
      "|2021-07-28 15:16:05|    2|\n",
      "|2021-07-28 15:16:06|    3|\n",
      "|2021-07-28 15:16:07|    3|\n",
      "|2021-07-28 15:16:08|    1|\n",
      "|2021-07-28 15:16:09|    2|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Edit: Not sure if we need this\n",
    "# We do a wordcount on Event_time, because at this point each Event_time row is one occurance for the most frequent hashtag\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as f\n",
    "df3 = df2 \\\n",
    "        .groupBy('Event_time') \\\n",
    "        .count() \\\n",
    "        .sort('Event_time', ascending=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|         Event_time|count|    Moving_Average|\n",
      "+-------------------+-----+------------------+\n",
      "|2021-07-28 14:52:41|    2|               2.0|\n",
      "|2021-07-28 14:52:42|    5|               3.5|\n",
      "|2021-07-28 14:52:43|    1|2.6666666666666665|\n",
      "|2021-07-28 15:15:53|    4|               4.0|\n",
      "|2021-07-28 15:15:54|    2|               3.0|\n",
      "|2021-07-28 15:15:55|    2|2.6666666666666665|\n",
      "|2021-07-28 15:15:56|    3|              2.75|\n",
      "|2021-07-28 15:15:57|    1|               2.4|\n",
      "|2021-07-28 15:15:58|    3|               2.5|\n",
      "|2021-07-28 15:15:59|    1|2.2857142857142856|\n",
      "|2021-07-28 15:16:00|    1|             2.125|\n",
      "|2021-07-28 15:16:01|    1|               2.0|\n",
      "|2021-07-28 15:16:02|    3|               2.1|\n",
      "|2021-07-28 15:16:03|    1|               2.0|\n",
      "|2021-07-28 15:16:04|    6|2.3333333333333335|\n",
      "|2021-07-28 15:16:05|    2|2.3076923076923075|\n",
      "|2021-07-28 15:16:06|    3| 2.357142857142857|\n",
      "|2021-07-28 15:16:07|    3|               2.4|\n",
      "|2021-07-28 15:16:08|    1|            2.3125|\n",
      "|2021-07-28 15:16:09|    2|2.2941176470588234|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL method - not sure if this is correct\n",
    "df3.createOrReplaceTempView(\"df3\")\n",
    "spark.sql(\n",
    "    \"\"\"SELECT *, mean(count) OVER (\n",
    "        ORDER BY CAST(Event_time AS timestamp) \n",
    "        RANGE BETWEEN INTERVAL 60 SECONDS PRECEDING AND CURRENT ROW\n",
    "     ) AS Moving_Average FROM df3\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results from 7a to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------+\n",
      "|End_of_Capturing_Window|Num_Tweets_Last_30_From_Newest_Tweet|\n",
      "+-----------------------+------------------------------------+\n",
      "|    2021-07-28 17:54:56|                                 231|\n",
      "+-----------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add timestamp to this record in case we want to make sense of the data in the hive table later\n",
    "# Also, reorder columns so timestamp is on left\n",
    "table_to_write = num_tweets_last_30.withColumn('End_of_Capturing_Window', lit(most_recent_tweet))\n",
    "table_to_write = table_to_write.select('End_of_Capturing_Window', 'Num_Tweets_Last_30_From_Newest_Tweet')\n",
    "table_to_write.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create db and table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS twitter_analysis\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    USE twitter_analysis\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets_last_30(\n",
    "        End_of_Capturing_Window String, \n",
    "        Num_Tweets_Last_30_From_Newest_Tweet String\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_to_write.write.mode(\"append\").format('hive').saveAsTable(\"twitter_analysis.tweets_last_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------+\n",
      "|End_of_Capturing_Window|Num_Tweets_Last_30_From_Newest_Tweet|\n",
      "+-----------------------+------------------------------------+\n",
      "|    2021-07-25 18:42:01|                                  84|\n",
      "|    2021-07-28 14:52:44|                                 178|\n",
      "|    2021-07-28 15:16:35|                                 412|\n",
      "|    2021-07-28 15:35:56|                                 328|\n",
      "|    2021-07-28 15:55:57|                                 323|\n",
      "|    2021-07-28 17:54:56|                                 231|\n",
      "+-----------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM tweets_last_30\n",
    "    ORDER BY End_of_Capturing_Window\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
